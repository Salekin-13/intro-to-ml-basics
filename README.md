# ğŸ§  ML Basics Showcase: Logistic Regression & MLP Classifiers

This project demonstrates foundational classification workflows using Logistic Regression and Multi-Layer Perceptron (MLP) on standard image datasets like MNIST and Fashion-MNIST. It is designed as part of a 2-week introductory deep learning curriculum to highlight key machine learning concepts through hands-on training, evaluation, and visualization.

---

## ğŸ“ Project Structure

```bash
intro-to-ml-basics/
â”œâ”€â”€ scripts/              # All reusable Python modules
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ notebooks/            # Interactive experiments and visualizations
â”‚   â”œâ”€â”€ 01_logistic_regression.ipynb
â”‚   â””â”€â”€ 02_mlp_classifier.ipynb
â”œâ”€â”€ results/              # Saved plots and trained model files
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ setup.py
```

---

## ğŸ“Œ Objectives

* Implement logistic regression and MLP from scratch using PyTorch
* Train on MNIST/Fashion-MNIST datasets
* Visualize training and validation performance
* Understand underfitting vs overfitting
* Apply techniques like regularization and dropout

---

## ğŸ› ï¸ Models Implemented

| Model               | Dataset       | Key Features                              |
| ------------------- | ------------- | ----------------------------------------- |
| Logistic Regression | MNIST, FMNIST | Linear classifier                         |
| MLP (2â€“3 layers)    | MNIST, FMNIST | ReLU, Dropout, Softmax, L2 regularization |

---

## ğŸ“Š Results

Here are sample training/validation curves and performance metrics:

| Model               | Dataset | Accuracy | Notes               |
| ------------------- | ------- | -------- | ------------------- |
| Logistic Regression | MNIST   | \~92%    | Simple linear model |
| MLP (2-layer)       | MNIST   | \~97%    | Better nonlinearity |
| MLP (with dropout)  | FMNIST  | \~89%    | Reduced overfitting |

> ğŸ“ˆ Training & validation accuracy/loss curves are visualized in the notebooks under `/notebooks`.

---

## ğŸ” Key Takeaways

* Logistic regression can perform well on simpler datasets like MNIST but struggles with more complex patterns.
* MLPs can model more complex decision boundaries with hidden layers.
* Dropout and L2 regularization are helpful in reducing overfitting, especially for deeper models.

---

## ğŸš€ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/intro-to-ml-basics.git
cd intro-to-ml-basics
```

### 2. Set up environment

```bash
pip install -e .
pip install -r requirements.txt
```

---

## âœ… Requirements

Python 3.8+
Recommended packages:

```
torch
torchvision
numpy
matplotlib
scikit-learn
tqdm
```

(Already included in `requirements.txt`)

---

## ğŸ“ Notes

* Built with educational clarity in mind â€” every function and model is well-commented and structured
* Easy to extend with CNN or other datasets
* No external training libraries used â€” just PyTorch and NumPy

---

## ğŸ’¡ Future Work

* Add CNN-based classifier
* Train on CIFAR-10
* Add TensorBoard integration
* Visualize learned weights

---

## ğŸ™‹â€â™€ï¸ Author

Made with care by \[Your Name]
Inspired by classic ML fundamentals + curiosity ğŸŒ±
